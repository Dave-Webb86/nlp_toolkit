{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical n-gram Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function is designed to identify technical n-grams (multi-word terms) within a text\n",
    "- n-grams provide more powerful token for use within nlp functions\n",
    "- Specifying technical n-grams ensures relevance rather than generically using all possible n-gram combinations which can add noise\n",
    "- Technique used is a semantic filter as defined by [Justeson & Katz, 1994](https://www.researchgate.net/publication/200044387_Technical_Terminology_Some_Linguistic_Properties_and_an_Algorithm_for_Identification_in_Text)\n",
    "- Limited to a maximum of tri-grams (3 word terms) as longer terms typically only appear in highly technical texts such as academic papers\n",
    "- Once identified n-grams are joined using underscores between the terms\n",
    "    - Suggest to save processed texts as a separate entity after processing to preserve the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "def get_ngram(text):\n",
    "\n",
    "    '''\n",
    "    Functions accepts a text string and identifies technical n-grams (up to\n",
    "    3-word terms. Once identified the n-grams are joined within the text via an \n",
    "    underscore to create more powerful tokens for use in further NLP work.\n",
    "    '''\n",
    "\n",
    "    # set up empty list to collect multi-word tokens\n",
    "    terms = []\n",
    "    # tokenize inputted text\n",
    "    tokens = word_tokenize(text)\n",
    "    # run semantic filter to identify multiword tokens and append to list\n",
    "    for x in range(len(tokens) - 2):\n",
    "        tokens = word_tokenize(text)\n",
    "        tag_tokens = nltk.pos_tag(tokens)\n",
    "        if tag_tokens[x][1] in ['NN', 'NNS', 'JJ'] and tag_tokens[x+1][1] in ['NN', 'NNS']:\n",
    "            terms.append((tokens[x], tokens[x+1]))   \n",
    "        if tag_tokens[x][1] in ['NN', 'NNS', 'JJ'] and tag_tokens[x+1][1] in ['NN', 'NNS', 'JJ'] \\\n",
    "        and tag_tokens[x+2][1] in ['NN', 'NNS']:\n",
    "            terms.append((tokens[x], tokens[x+1], tokens[x+2]))\n",
    "        if tag_tokens[x][1] in ['NN', 'NNS'] and tag_tokens[x+1][1] in ['IN'] \\\n",
    "        and tag_tokens[x+2][1] in ['NN', 'NNS']:\n",
    "            terms.append((tokens[x], tokens[x+1], tokens[x+2]))\n",
    "\n",
    "    # define function to stitch tokens back together \n",
    "    # and adjoin multiword tokens with an underscore\n",
    "    def collate_tech_terms(text):\n",
    "\n",
    "        tokens = word_tokenize(text, terms)\n",
    "        try:\n",
    "            for x in range(len(tokens) - 1):\n",
    "                if (tokens[x], tokens[x + 1], tokens[x + 2]) in terms:\n",
    "                    tokens[x] = str(tokens[x] + '_' + tokens[x+1] + '_' + tokens[x+2])\n",
    "                    tokens.remove(tokens[x + 1])\n",
    "                    tokens.remove(tokens[x + 1])\n",
    "                elif (tokens[x], tokens[x + 1]) in terms:\n",
    "                    tokens[x] = str(tokens[x] + '_' + tokens[x+1])\n",
    "                    tokens.remove(tokens[x + 1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "    # adjoin multiword tokens in user input text\n",
    "    text = collate_tech_terms(text, terms)\n",
    "\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
